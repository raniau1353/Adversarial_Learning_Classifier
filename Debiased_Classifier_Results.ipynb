{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raniau1353/Adversarial_Learning_Classifier/blob/main/Debiased_Classifier_Results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkKGEjWW-ZP1"
      },
      "source": [
        "# Read input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJqX1Yc-jUVJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import random_split, DataLoader, TensorDataset \n",
        "import torch.nn.functional as F \n",
        "from torch.optim import Adam \n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# read in datasets and transform to tensors\n",
        "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'])\n",
        "\n",
        "# define one hot encoder\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# embeddings\n",
        "income_labels = {' <=50K': 0, ' >50K': 1} \n",
        "workclass_labels = {' Private': 0, ' Self-emp-not-inc': 1, ' Local-gov': 2 , ' ?': 3, ' State-gov': 4, ' Self-emp-inc': 5, ' Federal-gov': 6, \n",
        "                    ' Without-pay': 7, ' Never-worked': 8}\n",
        "education_labels = {' HS-grad': 0, ' Some-college': 2, ' Bachelors': 3, ' Masters': 4, ' Assoc-voc': 5, ' 11th': 6, ' Assoc-acdm': 7, ' 10th': 8, \n",
        "                      ' 7th-8th': 9, ' Prof-school': 10, ' 9th': 11, ' 12th': 12, ' Doctorate': 13, ' 5th-6th': 14, ' 1st-4th': 15, ' Preschool': 16}\n",
        "marital_status_labels = {' Married-civ-spouse': 0, ' Never-married': 1, ' Divorced': 2, ' Separated': 3, ' Widowed': 4, ' Married-spouse-absent': 5, \n",
        "                         ' Married-AF-spouse': 6}\n",
        "occupation_labels = {' Prof-specialty': 0, ' Craft-repair': 1, ' Exec-managerial': 2, ' Adm-clerical': 3, ' Sales': 4, ' Other-service': 5, \n",
        "                     ' Machine-op-inspct': 6, ' ?': 7, ' Transport-moving': 8, ' Handlers-cleaners': 9, ' Farming-fishing': 10, ' Tech-support': 11, \n",
        "                     ' Protective-serv': 12, ' Priv-house-serv': 13, ' Armed-Forces': 14}\n",
        "relationship_labels = {' Husband': 0, ' Not-in-family': 1, ' Own-child': 2, ' Unmarried': 3, ' Wife': 4, ' Other-relative': 5}\n",
        "race_labels = {' White': 0, ' Black': 1, ' Asian-Pac-Islander': 2, ' Amer-Indian-Eskimo': 3, ' Other': 4}\n",
        "sex_labels = {' Male': 0, ' Female': 1}\n",
        "native_country_labels = {' United-States': 0, ' Mexico': 1, ' ?': 2, ' Philippines': 3, ' Germany': 4, ' Canada': 5, ' Puerto-Rico': 6, ' El-Salvador': 7, \n",
        "                         ' India': 8, ' Cuba': 9, ' England': 10, ' Jamaica': 11, ' South': 12, ' China': 13, ' Italy': 14, ' Dominican-Republic': 15, \n",
        "                         ' Vietnam': 16, ' Guatemala': 17, ' Japan': 18, ' Poland': 19, ' Columbia': 20, ' Taiwan': 21, ' Haiti': 22, ' Iran': 23, \n",
        "                         ' Portugal': 24, ' Nicaragua': 25, ' Peru': 26, ' France': 27, ' Greece': 28, ' Ecuador': 29, ' Ireland': 30, ' Hong': 31, \n",
        "                         ' Cambodia': 32, ' Trinadad&Tobago': 33, ' Laos': 34, ' Thailand': 34, ' Yugoslavia': 35, ' Outlying-US(Guam-USVI-etc)': 36, \n",
        "                         ' Honduras': 37, ' Hungary': 38, ' Scotland': 39, ' Holand-Netherlands': 40}\n",
        "\n",
        "# add output column for classifier\n",
        "df['Income_num'] = df['income']\n",
        "df.Income_num = [income_labels[item] for item in df.Income_num]\n",
        "\n",
        "# add output column for adversary\n",
        "df['Sex_num'] = df['sex']\n",
        "df.Sex_num = [sex_labels[item] for item in df.Sex_num]\n",
        "\n",
        "# add input columns\n",
        "df['Workclass_num'] = df['workclass']\n",
        "df['Education_num'] = df['education']\n",
        "df['Marital_Status_num'] = df['marital-status']\n",
        "df['Occupation_num'] = df['occupation']\n",
        "df['Relationship_num'] = df['relationship']\n",
        "df['Race_num'] = df['race']\n",
        "df['Native_Country_num'] = df['native-country']\n",
        "df['Capital_Gain_num'] = df['capital-gain']\n",
        "df['Capital_Loss'] = df['capital-loss']\n",
        "df['Hours_per_week'] = df['hours-per-week']\n",
        "\n",
        "df.Workclass_num = [workclass_labels[item] for item in df.Workclass_num]\n",
        "df.Education_num = [education_labels[item] for item in df.Education_num]\n",
        "df.Marital_Status_num = [marital_status_labels[item] for item in df.Marital_Status_num]\n",
        "df.Occupation_num = [occupation_labels[item] for item in df.Occupation_num]\n",
        "df.Relationship_num = [relationship_labels[item] for item in df.Relationship_num]\n",
        "df.Race_num = [race_labels[item] for item in df.Race_num]\n",
        "df.Native_Country_num = [native_country_labels[item] for item in df.Native_Country_num]\n",
        "\n",
        "# define input and output columns\n",
        "x1 = df.iloc[:, 17:24] \n",
        "x2 = df.iloc[:, 24:]\n",
        "onehot_input = encoder.fit_transform(x1)\n",
        "x1_tensor = torch.tensor(onehot_input)\n",
        "x2_tensor = torch.tensor(x2.to_numpy())\n",
        "\n",
        "x_var = torch.cat((x1_tensor, x2_tensor), -1)\n",
        "z_var = df.loc[:, 'Sex_num']\n",
        "y_var = df.loc[:, 'Income_num']\n",
        "\n",
        "print('\\nx_data format: ', x_var.shape, x_var.dtype)\n",
        "y_var = torch.tensor(y_var.to_numpy()) # Output format: torch.Size([150]) torch.int64 \n",
        "print('\\ny_data format: ', y_var.shape, y_var.dtype)\n",
        "z_var = torch.tensor(z_var.to_numpy()) # Adversary Output format:  torch.Size([32561]) torch.int64\n",
        "print('\\nz_data format: ', z_var.shape, z_var.dtype)\n",
        "\n",
        "data = TensorDataset(x_var, y_var, z_var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CeyvqCAAhDV"
      },
      "outputs": [],
      "source": [
        "# split to train, validate, and test sets using random_split\n",
        "# 50% of data used for training, 20% for validation, 30% testing\n",
        "number_rows = len(x_var)\n",
        "test_split = int(number_rows*0.3)\n",
        "validate_split = int(number_rows*0.2)\n",
        "train_split = number_rows - test_split - validate_split\n",
        "train_set, validate_set, test_set = random_split( \n",
        "    data, [train_split, validate_split, test_split]) \n",
        "\n",
        "# create dataloader to read data\n",
        "train_loader = DataLoader(train_set, batch_size = 32, shuffle = True)\n",
        "print(train_loader)\n",
        "validate_loader = DataLoader(validate_set, batch_size = 1)\n",
        "test_loader = DataLoader(test_set, batch_size = 1)\n",
        "\n",
        "print('# training samples:', number_rows)\n",
        "print('# batches:', len(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvCqs5ztJKzt"
      },
      "source": [
        "# Create model\n",
        "\n",
        "layers: (8, 24) -> (24, 24) -> (24, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgtsbPL2JNN6"
      },
      "outputs": [],
      "source": [
        "x_var_size = list(x_var.shape)[1] \n",
        "learning_rate = 0.01 \n",
        "y_var_size = len(income_labels)\n",
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "  def __init__(self, x_var_size, y_var_size):\n",
        "    super(Network, self).__init__()\n",
        "\n",
        "    self.layer1 = nn.Linear(x_var_size, 128)\n",
        "    self.layer2 = nn.Linear(128, 128)\n",
        "    self.layer3 = nn.Linear(128, y_var_size)\n",
        "\n",
        "  # output layer of the predictor is used as input to the adversary \n",
        "  def forward(self, x):\n",
        "    x1 = F.relu(self.layer1(x))\n",
        "    x2 = F.relu(self.layer2(x1)) \n",
        "    x3 = self.layer3(x2) \n",
        "    return x2, x3\n",
        "\n",
        "# instantiate the models\n",
        "model = Network(x_var_size, y_var_size)\n",
        "fair_model = Network(x_var_size, y_var_size)\n",
        "\n",
        "# Define execution device \n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"The model will be running on\", device, \"device\\n\") \n",
        "model.to(device)\n",
        "fair_model.to(device)\n",
        "\n",
        "# define a function to save models \n",
        "def saveModel():\n",
        "  path = \"./NetModel.pth\"\n",
        "  torch.save(model.state_dict(), path)\n",
        "\n",
        "def saveFairModel():\n",
        "  path2 = \"./NetModelFair.pth\"\n",
        "  torch.save(fair_model.state_dict(), path2)\n",
        "\n",
        "# define loss function as classification cross-entropy loss and an adam optimizer for the classifiers\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "optimizer_fair = Adam(fair_model.parameters(), lr=0.001, weight_decay=0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jADl_wWBxf1B"
      },
      "source": [
        "# Create Adversary Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO9JqWRixhzM"
      },
      "outputs": [],
      "source": [
        "z_var_size = len(sex_labels) \n",
        "\n",
        "class Adversary(nn.Module):\n",
        "\n",
        "  def __init__(self, z_var_size):\n",
        "    super(Adversary, self).__init__()\n",
        "\n",
        "    self.layer1 = nn.Linear(128, 128)\n",
        "    self.layer2 = nn.Linear(128, 128)\n",
        "    self.layer3 = nn.Linear(128, z_var_size)\n",
        "  \n",
        "  def forward(self, z):\n",
        "    z1 = F.relu(self.layer1(z))\n",
        "    z2 = F.relu(self.layer2(z1))\n",
        "    z3 = self.layer3(z2)\n",
        "    return z3\n",
        "\n",
        "adv = Adversary(z_var_size)\n",
        "adv_biased = Adversary(z_var_size)\n",
        "\n",
        "def saveAdversary():\n",
        "  adv_path = \"./AdvModel.pth\"\n",
        "  torch.save(adv.state_dict(), adv_path)\n",
        "\n",
        "def saveAdvBiased():\n",
        "  adv_path = \"./AdvBiasedModel.pth\"\n",
        "  torch.save(adv_biased.state_dict(), adv_path)\n",
        "\n",
        "loss_fn_adv = nn.CrossEntropyLoss()\n",
        "adv_optimizer = Adam(adv.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "adv_biased_optimizer = Adam(adv_biased.parameters(), lr=0.001, weight_decay=0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxjErEvjxoQS"
      },
      "source": [
        "# Pretrain fair classifier and adversary\n",
        "This function was used in old iterations and can be included in main or not. No significant difference in results were found. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xHWhVUYxlcf"
      },
      "outputs": [],
      "source": [
        "def pretrain(num_epochs):\n",
        "  predicted_outputs_clf = []\n",
        "\n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    for data in train_loader:\n",
        "      x_train, y_train, z_train = data\n",
        "\n",
        "      optimizer_fair.zero_grad()\n",
        "      adv_optimizer.zero_grad()\n",
        "\n",
        "      x_train = x_train.type(torch.FloatTensor)\n",
        "\n",
        "      predicted_outputs_clf_x2, predicted_outputs_clf_x3 = fair_model.forward(x_train)\n",
        "      predicted_outputs_adv = adv.forward(predicted_outputs_clf_x2)\n",
        "\n",
        "      train_loss_adv = loss_fn_adv(predicted_outputs_adv, z_train)\n",
        "      train_loss_adv.backward(retain_graph=True)\n",
        "\n",
        "      x_train = x_train.type(torch.LongTensor)\n",
        "      train_loss_clf = loss_fn(predicted_outputs_clf_x3, y_train)\n",
        "      train_loss_clf.backward()\n",
        "      \n",
        "      adv_optimizer.step()\n",
        "      optimizer_fair.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwJLdfnFNjJd"
      },
      "source": [
        "#Train biased classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ2QR1kDNhfh"
      },
      "outputs": [],
      "source": [
        "def train(num_epochs):\n",
        "  predicted_outputs_clf = []\n",
        "  \n",
        "  best_accuracy = 0.0\n",
        "\n",
        "  print(\"Start training...\")\n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    running_train_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "    running_val_loss = 0.0\n",
        "\n",
        "    adv_running_train_loss = 0.0\n",
        "    adv_running_accuracy = 0.0\n",
        "    adv_running_val_loss = 0.0\n",
        "\n",
        "    total = 0\n",
        "\n",
        "    for data in train_loader:\n",
        "      x_train, y_train, z_train = data \n",
        "      \n",
        "      # zero the optimizers\n",
        "      optimizer.zero_grad()\n",
        "      adv_biased_optimizer.zero_grad()\n",
        "\n",
        "      # obtain predictions from classifier and adversary\n",
        "      x_train = x_train.type(torch.FloatTensor)\n",
        "      predicted_outputs_clf_x2, predicted_outputs_clf_x3 = model.forward(x_train)\n",
        "      predicted_outputs_adv = adv_biased.forward(predicted_outputs_clf_x2)\n",
        "\n",
        "      # calculate training loss\n",
        "      train_loss_adv = loss_fn_adv(predicted_outputs_adv, z_train)\n",
        "      train_loss_adv.backward(retain_graph=True)\n",
        "\n",
        "      x_train = x_train.type(torch.LongTensor)\n",
        "      train_loss = loss_fn(predicted_outputs_clf_x3, y_train)\n",
        "\n",
        "      # backpropagate the loss\n",
        "      train_loss.backward()\n",
        "\n",
        "      # adjust parameters based on calculated gradients \n",
        "      adv_biased_optimizer.step()\n",
        "      optimizer.step()\n",
        "\n",
        "      # track the loss value\n",
        "      running_train_loss += train_loss.item()\n",
        "      adv_running_train_loss += train_loss_adv.item()\n",
        "    \n",
        "    # calculate training loss value\n",
        "    train_loss_value = running_train_loss/len(train_loader) \n",
        "    adv_train_loss_value = adv_running_train_loss/len(train_loader) \n",
        "\n",
        "    # validation loop\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "      adv.eval()\n",
        "      for data in validate_loader:\n",
        "        x_val, y_val, z_val = data\n",
        "        x_val = x_val.type(torch.FloatTensor)\n",
        "\n",
        "        predicted_outputs_clf_x2, predicted_outputs_clf_x3 = model.forward(x_val)\n",
        "\n",
        "        x_val = x_val.type(torch.LongTensor)\n",
        "        val_loss = loss_fn(predicted_outputs_clf_x3, y_val)\n",
        "\n",
        "        predicted_outputs_adv = adv_biased.forward(predicted_outputs_clf_x2)\n",
        "        adv_val_loss = loss_fn_adv(predicted_outputs_adv, z_val)\n",
        "\n",
        "        _, predicted = torch.max(predicted_outputs_clf_x3, 1)\n",
        "        _, adv_predicted = torch.max(predicted_outputs_adv, 1)\n",
        "\n",
        "        running_val_loss += val_loss.item()\n",
        "        adv_running_val_loss += adv_val_loss.item()\n",
        "\n",
        "        total += y_val.size(0)\n",
        "\n",
        "        running_accuracy += (predicted == y_val).sum().item()\n",
        "        adv_running_accuracy += (adv_predicted == z_val).sum().item()\n",
        "    \n",
        "    # calculate validation loss value\n",
        "    val_loss_value = running_val_loss/len(validate_loader)\n",
        "    adv_val_loss_value = adv_running_val_loss/len(validate_loader)\n",
        "\n",
        "    # accuracy = # correct predictions in validation batch / # total predictions\n",
        "    accuracy = (100 * running_accuracy / total)\n",
        "    adv_accuracy = (100 * adv_running_accuracy / total)\n",
        "\n",
        "    # save model if best so far\n",
        "    if accuracy > best_accuracy:\n",
        "      saveModel()\n",
        "      best_accuracy = accuracy\n",
        "\n",
        "    # print statistics of epoch\n",
        "    print('Completed training batch', epoch, '\\nBiased Model Training Loss is: %.4f' %train_loss_value, \n",
        "          'Biased Model Validation Loss is: %.4f' %val_loss_value, 'Biased Model Accuracy is %d %%' % (accuracy)) \n",
        "    print('Adversary Training Loss is: %.4f' %adv_train_loss_value, \n",
        "            'Adversary Validation Loss is: %.4f' %adv_val_loss_value, 'Adversary Accuracy is %d %%' % (adv_accuracy))\n",
        "  \n",
        "  saveAdvBiased()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-go4qdstxw5l"
      },
      "source": [
        "# Start debiasing Training\n",
        "This function follows the process of updating the weights of the classifier as seen in the paper (Figure 2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5s6hyKexvzm"
      },
      "outputs": [],
      "source": [
        "def debias(num_epochs):\n",
        "  adversary_loss_weight = 0.1\n",
        "  best_clf_accuracy = 0.0\n",
        "\n",
        "  print(\"\\nStart debiasing training...\")\n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    clf_running_train_loss = 0.0\n",
        "    clf_running_accuracy = 0.0\n",
        "    clf_running_val_loss = 0.0\n",
        "\n",
        "    adv_running_train_loss = 0.0\n",
        "    adv_running_accuracy = 0.0\n",
        "    adv_running_val_loss = 0.0\n",
        "\n",
        "    total = 0\n",
        "\n",
        "    for data in train_loader:\n",
        "      x_train, y_train, z_train = data\n",
        "\n",
        "      optimizer_fair.zero_grad()\n",
        "      adv_optimizer.zero_grad()\n",
        "\n",
        "      x_train = x_train.type(torch.FloatTensor)\n",
        "      predicted_outputs_clf_x2, predicted_outputs_clf_x3 = fair_model.forward(x_train)\n",
        "\n",
        "      train_loss_clf = loss_fn(predicted_outputs_clf_x3, y_train)\n",
        "      train_loss_clf.backward(retain_graph=True)\n",
        "      clf_running_train_loss += train_loss_clf.item() # calc\n",
        "\n",
        "      # dW_LP (in paper)\n",
        "      clf_grad = [torch.clone(par.grad.detach()) for par in fair_model.parameters()]\n",
        "\n",
        "      optimizer_fair.zero_grad()\n",
        "      adv_optimizer.zero_grad()\n",
        "\n",
        "      predicted_outputs_adv = adv.forward(predicted_outputs_clf_x2)\n",
        "      train_loss_adv = loss_fn_adv(predicted_outputs_adv, z_train)\n",
        "      train_loss_adv.backward(retain_graph=True)\n",
        "      adv_running_train_loss += train_loss_adv.item()\n",
        "\n",
        "      # dW_LA (in paper)\n",
        "      adv_grad = [torch.clone(par.grad.detach()) for par in fair_model.parameters()]\n",
        "\n",
        "      for i,par in enumerate(fair_model.parameters()):\n",
        "        unit_adversary_grad = adv_grad[i] / (torch.norm(adv_grad[i]) + torch.finfo(float).eps)\n",
        "        # projection proj_{dW_LA}(dW_LP) (in paper)\n",
        "        proj = torch.sum(unit_adversary_grad * clf_grad[i])\n",
        "        par.grad = clf_grad[i] - (proj * unit_adversary_grad) - (adversary_loss_weight * adv_grad[i])\n",
        "\n",
        "      optimizer_fair.step()\n",
        "      adv_optimizer.step()\n",
        "    \n",
        "    # calculate training loss value\n",
        "    clf_train_loss_value = clf_running_train_loss/len(train_loader) \n",
        "    adv_train_loss_value = adv_running_train_loss/len(train_loader) \n",
        "\n",
        "    with torch.no_grad():\n",
        "      fair_model.eval()\n",
        "      adv_biased.eval()\n",
        "      for data in validate_loader:\n",
        "        x_val, y_val, z_val = data\n",
        "\n",
        "        x_val = x_val.type(torch.FloatTensor)\n",
        "        predicted_outputs_clf_x2, predicted_outputs_clf_x3 = fair_model.forward(x_val)\n",
        "        clf_val_loss = loss_fn(predicted_outputs_clf_x3, y_val)\n",
        "\n",
        "        predicted_outputs_adv = adv.forward(predicted_outputs_clf_x2)\n",
        "        adv_val_loss = loss_fn_adv(predicted_outputs_adv, z_val)\n",
        "\n",
        "        _, clf_predicted = torch.max(predicted_outputs_clf_x3, 1)\n",
        "        _, adv_predicted = torch.max(predicted_outputs_adv, 1)\n",
        "\n",
        "        clf_running_val_loss += clf_val_loss.item()\n",
        "        adv_running_val_loss += adv_val_loss.item()\n",
        "\n",
        "        total += y_val.size(0)\n",
        "\n",
        "        clf_running_accuracy += (clf_predicted == y_val).sum().item()\n",
        "        adv_running_accuracy += (adv_predicted == z_val).sum().item()\n",
        "      \n",
        "    # calculate validation loss value\n",
        "    clf_val_loss_value = clf_running_val_loss/len(validate_loader)\n",
        "    adv_val_loss_value = adv_running_val_loss/len(validate_loader)\n",
        "\n",
        "    # accuracy = # correct predictions in validation batch / # total predictions\n",
        "    clf_accuracy = (100 * clf_running_accuracy / total)\n",
        "    adv_accuracy = (100 * adv_running_accuracy / total)\n",
        "\n",
        "    if clf_accuracy > best_clf_accuracy:\n",
        "      saveFairModel()\n",
        "      best_clf_accuracy = clf_accuracy\n",
        "\n",
        "    # print statistics of epoch\n",
        "    print('Completed training batch', epoch, '\\nDebiased Model Training Loss is: %.4f' %clf_train_loss_value, \n",
        "            'Debiased Model Validation Loss is: %.4f' %clf_val_loss_value, 'Debiased Model Accuracy is %d %%' % (clf_accuracy))\n",
        "    print('Adversary Training Loss is: %.4f' %adv_train_loss_value, \n",
        "            'Adversary Validation Loss is: %.4f' %adv_val_loss_value, 'Adversary Accuracy is %d %%' % (adv_accuracy)) \n",
        "  \n",
        "  saveAdversary()\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yctQ8nWczu8c"
      },
      "source": [
        "# Test both classifiers\n",
        "This function obtains results from the biased and debiased classifiers and adversaries. The measure of achieving equality of odds is seen by a decrease in difference between FP (False Positive) and FN (False Negative) for male versus female subgroups as predicted by the classifier. While the classifier is still predicting income, the information about sex is provided from the corresponding array containing the protected variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME8Wd36nzqHZ"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# function to test the model\n",
        "# Goal: decrease difference between FPR male and FPR female, FNR male and FNR female\n",
        "def test(classifier_path, adversary_path, model_type):\n",
        "  classifier_pred_female = []\n",
        "  classifier_actual_female = []\n",
        "\n",
        "  classifier_pred_male = []\n",
        "  classifier_actual_male = []\n",
        "\n",
        "  # load the models and adversaries\n",
        "  model = Network(x_var_size, y_var_size)\n",
        "  model.load_state_dict(torch.load(classifier_path)) \n",
        "\n",
        "  adversary = Adversary(z_var_size)\n",
        "  adversary.load_state_dict(torch.load(adversary_path))\n",
        "  \n",
        "  running_accuracy = 0;\n",
        "  total_clf = 0\n",
        "\n",
        "  # keep track of correct and incorrect predictions\n",
        "  labels_length = len(sex_labels)\n",
        "  labels_correct = list(0. for i in range(labels_length))\n",
        "  labels_incorrect = list(0. for i in range(labels_length))\n",
        "  labels_total = list(0. for i in range(labels_length))\n",
        "\n",
        "  clf_FP = list(0. for i in range(labels_length))\n",
        "  clf_FP_total = list(0. for i in range(labels_length))\n",
        "  clf_FN = list(0. for i in range(labels_length))\n",
        "  clf_FN_total = list(0. for i in range(labels_length))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "      x_test, y_test, z_test = data\n",
        "\n",
        "      x_test = x_test.type(torch.FloatTensor)\n",
        "      predicted_outputs_clf_x2, predicted_outputs_clf_x3 = model(x_test)\n",
        "      predicted_outputs_adv = adversary(predicted_outputs_clf_x2)\n",
        "\n",
        "      _, predicted_clf = torch.max(predicted_outputs_clf_x3, 1) \n",
        "      _, predicted_adv = torch.max(predicted_outputs_adv, 1)\n",
        "\n",
        "      total_clf += y_test.size(0)\n",
        "      running_accuracy += (predicted_clf == y_test).sum().item()\n",
        "\n",
        "      # adversary results\n",
        "      label_correct_running = (predicted_adv == z_test).squeeze()\n",
        "      label = int(z_test[0]) # Male: 0, Female: 1\n",
        "      if label_correct_running.item():\n",
        "        labels_correct[label] += 1\n",
        "      else:\n",
        "        labels_incorrect[label] += 1\n",
        "\n",
        "      # accuracies of classifier for each sex group\n",
        "      # FPR = #(pred 1, true 0) / (#(pred 1, true 0) + #(pred 0, true 0))\n",
        "      # FNR = #(pred 0, true 1) / (#(pred 0, true 1) + #(pred 1, true 1))\n",
        "      clf_correct_running = (predicted_clf == y_test).squeeze()\n",
        "\n",
        "      if not clf_correct_running.item(): # incorrect prediction\n",
        "        if predicted_clf.item(): # predicted = 1\n",
        "          clf_FP[label] += 1\n",
        "          clf_FP_total[label] += 1\n",
        "        else: # predicted = 0\n",
        "          clf_FN[label] += 1\n",
        "          clf_FN_total[label] += 1\n",
        "      else: # correct prediction\n",
        "        if not predicted_clf.item() and not y_test.item(): #(pred 0, true 0))\n",
        "          clf_FP_total[label] += 1\n",
        "        else: #(pred 1, true 1))\n",
        "          clf_FN_total[label] += 1\n",
        "\n",
        "      # create arrays for confusion matrices\n",
        "      if label == 0: # male subgroup\n",
        "        classifier_pred_male.append(predicted_clf.item())\n",
        "        classifier_actual_male.append(y_test.item())\n",
        "      else: # female subgroup\n",
        "        classifier_pred_female.append(predicted_clf.item())\n",
        "        classifier_actual_female.append(y_test.item())\n",
        "      \n",
        "      labels_total[label] += 1\n",
        "    \n",
        "    print('Accuracy of the', model_type, 'model based on the test set of', test_split ,\n",
        "          'x_test is: %d %%\\n' % (100 * running_accuracy / total_clf))\n",
        "    \n",
        "    # print confusion matrices for each sex\n",
        "    print(model_type, 'Male Subgroup Confusion Matrix')\n",
        "    confusion_matrix_male = metrics.confusion_matrix(classifier_actual_male, classifier_pred_male, labels=[0,1])\n",
        "    cm_display_male = metrics.ConfusionMatrixDisplay(confusion_matrix_male, display_labels = [False, True])\n",
        "    cm_display_male.plot()\n",
        "    plt.show()\n",
        "\n",
        "    print(model_type, 'Female Subgroup Confusion Matrix')\n",
        "    confusion_matrix_female = metrics.confusion_matrix(classifier_actual_female, classifier_pred_female, labels=[0, 1])\n",
        "    cm_display_female = metrics.ConfusionMatrixDisplay(confusion_matrix_female, display_labels = [False, True])\n",
        "    cm_display_female.plot()\n",
        "    plt.show()\n",
        "\n",
        "    # print adversary and classifier results \n",
        "    label_list = list(sex_labels.keys())\n",
        "    for i in range(z_var_size):\n",
        "      print(model_type, 'Adversary correctly predicting%5s: %2d %%' % (label_list[i], 100 * labels_correct[i] / labels_total[i])) \n",
        "      print(model_type, 'Adversary incorrectly predicting%5s: %2d %%\\n' % (label_list[i], 100 * labels_incorrect[i] / labels_total[i]))\n",
        "      print(model_type, 'Classifier False Positive income prediction on%5s subgroup: %5.4f ' % (label_list[i], round(clf_FP[i]/clf_FP_total[i], 4))) # FPR = #(pred 1, true 0) / (#(pred 1, true 0) + #(pred 0, true 0))\n",
        "      print(model_type, 'Classifier False Negative income prediction on%5s subgroup: %5.4f \\n' % (label_list[i], round(clf_FN[i]/clf_FN_total[i], 4))) # FNR = #(pred 0, true 1) / (#(pred 0, true 1) + #(pred 1, true 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Je6F9N61JwW"
      },
      "source": [
        "# main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K4yCuRw1JHH"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  num_epochs = 25\n",
        "  pretrain(num_epochs) # optional\n",
        "  # train\n",
        "  train(num_epochs)\n",
        "  debias(num_epochs)\n",
        "  print('Finished Training\\n') \n",
        "  # test\n",
        "  test(\"NetModel.pth\", \"AdvBiasedModel.pth\", \"Biased\")\n",
        "  test(\"NetModelFair.pth\", \"AdvModel.pth\", \"Debiased\") \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}